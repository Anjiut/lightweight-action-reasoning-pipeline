A Lightweight Multimodal Human Action Reasoning Pipeline for Embodied Agents
This repository presents a lightweight, end-to-end research prototype for human action understanding and semantic reasoning, designed as a foundational component for future embodied agents.

Given short human action videos, the system performs probabilistic pose-based action recognition and subsequently applies LLM-based reasoning to infer human intent, environmental context, and task-level plans in a structured and interpretable manner.

The project emphasizes modularity, uncertainty awareness, and reasoning transparency, explicitly bridging low-level perception with high-level symbolic reasoning.

â¸»

ğŸ“– Overview

Understanding human actions and intentions is a core challenge in embodied intelligence. A key difficulty lies in reasoning under perception uncertainty while maintaining coherent task-level understanding.

This project explores a modular pipeline that:
	1.	Models perception uncertainty via pose estimation and a probabilistic MLP classifier that outputs action confidence.
	2.	Performs semantic and temporal reasoning using LLM-based agents to infer intent, affordances, and future steps.

The resulting system serves as an interpretable research prototype for studying action reasoning in embodied-agent settings.

â¸»

âš™ï¸ Pipeline

Training Stage

Video input
â†’ frame extraction
â†’ pose estimation
â†’ feature vectorization
â†’ probabilistic MLP training

Inference & Reasoning Stage

New video
â†’ pose extraction
â†’ MLP action prediction (with confidence)
â†’ single-action reasoning (LLM)
â†’ temporal / task-level reasoning (LLM)
â†’ structured JSON output

Each module is independently analyzable and replaceable, enabling flexible research extensions.

â¸»

ğŸ·ï¸ Supported Action Labels

This prototype uses a small, interpretable action set to prioritize reasoning analysis:
	â€¢	open_door
	â€¢	pick_book
	â€¢	pour_water
	â€¢	walk_stop

These categories are intentionally limited to support rapid experimentation and clear semantic interpretation.

â¸»

ğŸ“‚ Project Structure (Core Components)

Perception & Preprocessing
	â€¢	extract_frames.py â€“ extract frames from training videos
	â€¢	extract_new_frames.py â€“ extract frames from test videos
	â€¢	extract_pose.py â€“ OpenPifPaf pose estimation (training)
	â€¢	extract_new_pose.py â€“ OpenPifPaf pose estimation (testing)
	â€¢	build_dataset.py â€“ convert keypoints to fixed-length vectors

Learning
	â€¢	train_mlp.py â€“ train probabilistic MLP classifier
	â€¢	mlp_action_model.pkl â€“ trained model
	â€¢	pose_scaler.pkl â€“ feature scaler

Reasoning & Integration
	â€¢	reasoning_agent.py â€“ LLM-based intent and temporal reasoning
	â€¢	full_pipeline.py â€“ end-to-end pipeline (video â†’ JSON reasoning)

Visualization & Utilities
	â€¢	visualize_frames.py â€“ frame inspection
	â€¢	visualize_pose.py â€“ pose overlay debugging

Note: Video data, extracted frames, and keypoints are not included in the repository.

â¸»

ğŸ’» Installation

This project was tested with Python 3.10.

Install dependencies using: pip install -r requirements.txt

ğŸ”‘ OpenAI API Key

The reasoning agent requires an OpenAI API key.

Set it via environment variable: export OPENAI_API_KEY="YOUR_API_KEY"

ğŸš€ How to Run

1. Training Pipeline

Place training videos under videos/, named by action label:

open_door.mp4
pick_book.mp4
pour_water.mp4
walk_stop.mp4

Run the following in order:

python extract_frames.py
python extract_pose.py
python build_dataset.py
python train_mlp.py

This produces the trained classifier and scaler.

â¸»

2. Inference & Reasoning Pipeline

Place test videos under new_videos/, then run:

python extract_new_frames.py
python extract_new_pose.py
python full_pipeline.py

The system outputs:
	â€¢	Majority-vote action predictions with confidence
	â€¢	Per-action semantic reasoning (JSON)
	â€¢	Temporal task-level reasoning over action sequences

â¸»

ğŸ“„ Output Format (Example)

The reasoning module produces structured JSON suitable for downstream agent integration:

{
  "action": "pour_water",
  "confidence": 0.92,
  "reasoning": {
    "intent": "To fill the cup with water for drinking",
    "next_step": "Put down the kettle",
    "environment": {
      "scene_type": "Kitchen",
      "key_objects": ["Kettle", "Cup"]
    }
  }
}

âš ï¸ Limitations & Future Work

Current limitations:
	â€¢	Assumes a single visible human per video
	â€¢	Environment understanding is inferred symbolically rather than visually grounded
	â€¢	Action taxonomy is intentionally small for prototype clarity

Planned extensions:
	â€¢	Bayesian uncertainty modeling and OOD detection
	â€¢	Visual grounding via vision-language models
	â€¢	Multi-agent tracking and interaction reasoning

â¸»

ğŸ“œ License

MIT License.


